{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f63612-e5d2-4d03-b841-2e838d0f12d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../utils/mount_configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464ca3c8-cdff-4a71-a9d1-bcd963eaa9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../utils/incremental_load\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8887f2-fedc-41de-a68f-defa16d95963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_financial_df = spark.read.format('delta') \\\n",
    "                                 .load(f'{processed_folder_path}/tickers_financial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0183a690-510a-4231-8c51-03bb1973b11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_price_df = spark.read.format('delta') \\\n",
    "                             .load(f'{processed_folder_path}/tickers_price') \\\n",
    "                             .select('ticker', 'close_price', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ad2db7-5513-4bc3-b68e-3603af3cebb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_details_df = spark.read.format('delta') \\\n",
    "                               .load(f'{processed_folder_path}/tickers_details') \\\n",
    "                               .drop('market_cap', 'total_employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b64ad93-7de7-41e2-9e53-9854ea8be9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_dividend_df = spark.read.format('delta') \\\n",
    "                                .load(f'{processed_folder_path}/tickers_dividend') \\\n",
    "                                .select(\"ticker\", \"pay_date\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b74277-f433-4b11-bb00-2551967621bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, desc, lead, col, when, sum, max\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Define windows\n",
    "window_spec = Window.partitionBy(\"ticker\").orderBy(desc(\"year\"))\n",
    "count_window = Window.partitionBy(\"ticker\")\n",
    "\n",
    "# Count continuous dividend years for each company\n",
    "tickers_dividend_df = tickers_dividend_df \\\n",
    "    .withColumn(\"year\", year(\"pay_date\")) \\\n",
    "    .select(\"ticker\", \"year\") \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"next_year\", lead(\"year\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"is_continuous\", when(col(\"year\") - col(\"next_year\") == 1, 1).otherwise(0)) \\\n",
    "    .withColumn(\"continuous_sum\", sum(\"is_continuous\").over(count_window) + 1)\n",
    "\n",
    "# Extract final result for each company\n",
    "tickers_cont_div_years_df = tickers_dividend_df.groupBy(\"ticker\") \\\n",
    "                                               .agg(max(\"continuous_sum\").alias(\"continuous_dividend_years\"))\n",
    "\n",
    "display(tickers_cont_div_years_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9dcc57-0daf-400e-9588-690d4aea1fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tickers_financial_df.filter(\"ticker =='PYPL' and fiscal_period == 'FY'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2eaeab-a7c8-48ba-a208-e50ffba1f1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, round, isnan, row_number, abs\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window for tickers, sorted by fiscal year\n",
    "window_spec = Window.partitionBy(\"cagr_ticker\").orderBy(\"fiscal_year\")\n",
    "\n",
    "# Filter data for fiscal year \"FY\", add lag and calculate CAGR\n",
    "net_income_3_year_cagr = tickers_financial_df.filter(col(\"fiscal_period\") == \"FY\") \\\n",
    "    .withColumnRenamed(\"ticker\", \"cagr_ticker\") \\\n",
    "    .withColumn(\"net_income_lag_3y\", lag(\"net_income_loss\", 2).over(window_spec)) \\\n",
    "    .withColumn(\n",
    "        \"net_income_3_year_cagr\",\n",
    "        ((col(\"net_income_loss\") - col(\"net_income_lag_3y\")) / 3) / abs(col(\"net_income_lag_3y\"))\n",
    "    ) \\\n",
    "    .withColumn(\"net_income_3_year_cagr_pct\", round(col(\"net_income_3_year_cagr\") * 100, 2)) \\\n",
    "    .filter(col(\"net_income_3_year_cagr_pct\").isNotNull()) \\\n",
    "    .select(\"cagr_ticker\", \"fiscal_year\", \"net_income_loss\", \"net_income_lag_3y\", \"net_income_3_year_cagr_pct\")\n",
    "\n",
    "# Window to select the latest record (highest fiscal_year) per ticker\n",
    "latest_window = Window.partitionBy(\"cagr_ticker\").orderBy(col(\"fiscal_year\").desc())\n",
    "\n",
    "# Add row number and select only the first (latest) one\n",
    "latest_net_income_cagr = net_income_3_year_cagr \\\n",
    "    .withColumn(\"row_num\", row_number().over(latest_window)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .select(\"cagr_ticker\", \"net_income_3_year_cagr_pct\")\n",
    "\n",
    "display(net_income_3_year_cagr)\n",
    "display(latest_net_income_cagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2968af4b-bbc9-4cf7-93ba-7085f2d9032e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, collect_set, array_contains\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"ticker\").orderBy(col(\"start_date\").desc())\n",
    "\n",
    "#licze które tickety sa poprawne aby użyć kwartalne daty a dla których użyć FY\n",
    "valid_tickers_for_quarters = tickers_financial_df.filter(col(\"fiscal_period\").like(\"Q%\")) \\\n",
    "                                                 .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                                                 .filter((col(\"row_num\") <= 4)) \\\n",
    "                                                 .groupBy(\"ticker\") \\\n",
    "                                                 .agg(collect_set(\"fiscal_period\").alias(\"quarters\")) \\\n",
    "                                                 .filter(array_contains(col(\"quarters\"), \"Q1\") &\n",
    "                                                            array_contains(col(\"quarters\"), \"Q2\") &\n",
    "                                                            array_contains(col(\"quarters\"), \"Q3\") &\n",
    "                                                            array_contains(col(\"quarters\"), \"Q4\"))\n",
    "\n",
    "display(valid_tickers_for_quarters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6727fc-fac7-4a40-badc-a6ff09288841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"ticker\").orderBy(desc(\"start_date\"))\n",
    "\n",
    "tickers_financial_fy_df = tickers_financial_df.filter(col(\"fiscal_period\") == \"FY\") \\\n",
    "                                              .join(valid_tickers_for_quarters, \"ticker\", \"left_anti\") \\\n",
    "                                              .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                                              .filter(col(\"row_num\") == 1) \\\n",
    "                                              .withColumn(\"net_income_loss_4q_sum\", col(\"net_income_loss\")) \\\n",
    "                                              .withColumnRenamed(\"interest_expense_operating\", \"interest_expense_operating_4q_sum\") \\\n",
    "                                              .withColumnRenamed(\"income_tax_expense_benefit\", \"income_tax_expense_benefit_4q_sum\") \\\n",
    "                                              .withColumnRenamed(\"revenues\", \"revenues_4q_sum\") \\\n",
    "                                              .select('ticker', 'net_income_loss', 'net_income_loss_4q_sum', 'liabilities', \n",
    "                                                    'equity', 'assets', 'current_assets', 'current_liabilities', 'long_term_debt', \n",
    "                                                    'interest_expense_operating_4q_sum', 'income_tax_expense_benefit_4q_sum',\n",
    "                                                    'depreciation_and_amortization', 'revenues_4q_sum', 'fiscal_period',\n",
    "                                                    'fiscal_year', 'start_date', 'end_date')                                              \n",
    "\n",
    "display(tickers_financial_fy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afafc61-85cd-49bb-a57f-c7912c745214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, desc, year, current_date\n",
    "\n",
    "window_spec = Window.partitionBy('ticker').orderBy('end_date').rowsBetween(-3, 0)\n",
    "\n",
    "last_4_quarters_window_spec = Window.partitionBy('ticker').orderBy(desc('end_date'))\n",
    "\n",
    "# Sum 4 quarters as it is always calculated annually\n",
    "tickers_financial_quarter_df = tickers_financial_df.filter(\"fiscal_period != 'FY' \") \\\n",
    "                                                   .join(valid_tickers_for_quarters, \"ticker\", \"inner\") \\\n",
    "                                                   .withColumn('net_income_loss_4q_sum', sum('net_income_loss').over(window_spec)) \\\n",
    "                                                   .withColumn('interest_expense_operating_4q_sum', sum('interest_expense_operating').over(window_spec)) \\\n",
    "                                                   .withColumn('income_tax_expense_benefit_4q_sum', sum('income_tax_expense_benefit').over(window_spec)) \\\n",
    "                                                   .withColumn('revenues_4q_sum', sum('revenues').over(window_spec)) \\\n",
    "                                                   .withColumn('row_num', row_number().over(last_4_quarters_window_spec)) \\\n",
    "                                                   .filter(col(\"row_num\") <= 4) \\\n",
    "                                                   .select('ticker', 'net_income_loss', 'net_income_loss_4q_sum', 'liabilities', \n",
    "                                                        'equity', 'assets', 'current_assets', 'current_liabilities', 'long_term_debt', \n",
    "                                                        'interest_expense_operating_4q_sum', 'income_tax_expense_benefit_4q_sum',\n",
    "                                                        'depreciation_and_amortization', 'revenues_4q_sum', 'fiscal_period',\n",
    "                                                        'fiscal_year', 'start_date', 'end_date')\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fe3b53-aabe-4a57-8430-6c2cf4b6bc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tickers_financial_quarter_df.filter(\"ticker == 'AAPL'\").orderBy(desc('start_date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2807df39-78b4-4c25-9498-73a1b16bb980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_financial_merged_df = tickers_financial_quarter_df.union(tickers_financial_fy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31899343-c0dd-427a-a94c-1a9b9196b2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tickers_financial_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d86755-20b8-41f8-be0e-80ebc2fcced3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tickers_financial_merged_df.select(\"ticker\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ebc14f-f054-4d8b-8386-019fa8cd33d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_net_income_cagr_df = tickers_financial_merged_df.join(latest_net_income_cagr,\n",
    "                                                                  (tickers_financial_merged_df.ticker == latest_net_income_cagr.cagr_ticker),\n",
    "                                                            \"left\") \\\n",
    "                                                            .drop(latest_net_income_cagr.cagr_ticker)\n",
    "\n",
    "display(merged_net_income_cagr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb334b64-e302-4413-830c-2d2aed7ee5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(merged_net_income_cagr_df.select(merged_net_income_cagr_df.ticker).distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c681980-deb6-47c6-bf77-63faf49ebdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_dividends_df = merged_net_income_cagr_df.join(tickers_cont_div_years_df,\n",
    "                                    (merged_net_income_cagr_df.ticker == tickers_cont_div_years_df.ticker),\n",
    "                                    'left') \\\n",
    "                                   .drop(tickers_cont_div_years_df.ticker)\n",
    "\n",
    "display(merged_dividends_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e8a00c-54e3-483b-a4b7-a27044a08d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(merged_dividends_df.filter(\"ticker == 'COST'\"))\n",
    "display(merged_dividends_df.select(\"ticker\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4575705e-41d8-4fc0-8dea-34d6bca2ebfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "details_df = tickers_details_df.orderBy('date')\n",
    "merged_dividends_df = merged_dividends_df.orderBy('start_date')\n",
    "\n",
    "\n",
    "merged_details_df = details_df.join(merged_dividends_df, \n",
    "                            (details_df.ticker == merged_dividends_df.ticker) & (details_df.date >= merged_dividends_df.start_date) & \n",
    "                                (details_df.date <= merged_dividends_df.end_date),\n",
    "                            'inner')\n",
    "\n",
    "merged_details_df = merged_details_df \\\n",
    "    .withColumn(\"EPS\", col(\"net_income_loss_4q_sum\") / col(\"weighted_shares_outstanding\")) \\\n",
    "    .withColumn(\"BVPS\", col(\"equity\") / col(\"weighted_shares_outstanding\")) \\\n",
    "    .withColumn(\"debt_rate\", col(\"liabilities\") / col(\"assets\")) \\\n",
    "    .withColumn(\"D/E\", col(\"long_term_debt\") / col(\"equity\")) \\\n",
    "    .withColumn(\"current_liquidity_rate\", col(\"current_assets\") / col(\"current_liabilities\")) \\\n",
    "    .withColumn(\"ROE\", col(\"net_income_loss_4q_sum\") / col(\"equity\")) \\\n",
    "    .withColumn(\"ROA\", col(\"net_income_loss_4q_sum\") / col(\"assets\")) \\\n",
    "    .withColumn(\"EBIDTA\", \n",
    "        col(\"net_income_loss_4q_sum\") + \n",
    "        col(\"interest_expense_operating_4q_sum\") + \n",
    "        col(\"income_tax_expense_benefit_4q_sum\") + \n",
    "        col(\"depreciation_and_amortization\")) \\\n",
    "    .withColumn(\"net_profit_margin\", col(\"net_income_loss_4q_sum\") / col(\"revenues_4q_sum\")) \\\n",
    "    .withColumn(\"debt_EBIDTA\", col(\"long_term_debt\") / col(\"EBIDTA\")) \\\n",
    "    .select(details_df[\"ticker\"], \"name\", \"type\", \"primary_exchange\", \"weighted_shares_outstanding\", \"EPS\", \n",
    "            \"BVPS\", \"debt_rate\", \"revenues_4q_sum\", \"D/E\", \"current_liquidity_rate\", \"ROE\", \"ROA\", \"EBIDTA\", \"net_profit_margin\", \n",
    "            \"long_term_debt\", \"debt_EBIDTA\", \"continuous_dividend_years\", \"net_income_3_year_cagr_pct\", \"date\", \"fiscal_period\", \n",
    "            \"fiscal_year\", \"start_date\", \"end_date\")\n",
    "\n",
    "display(merged_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b25ddb6-5fd1-4242-b36e-360a35eca358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(merged_details_df.filter(\"ticker == 'COST'\").orderBy('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2850b345-78cc-47a3-8188-4c6e72343f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_price_df = tickers_price_df.orderBy('date')\n",
    "merged_details_df = merged_details_df.orderBy('start_date')\n",
    "\n",
    "merged_price_df = tickers_price_df.join(merged_details_df, \n",
    "                           (tickers_price_df.ticker == merged_details_df.ticker) & (tickers_price_df.date == merged_details_df.date),    \n",
    "                           'inner') \\\n",
    "                           .drop(tickers_price_df.ticker, tickers_price_df.date) \\\n",
    "                           .orderBy(merged_details_df['date'])\n",
    "                           \n",
    "\n",
    "display(merged_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61f6b14-5eb3-4bfb-8eb6-339ae7e23e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = merged_price_df.withColumn(\"market_cap\", col(\"weighted_shares_outstanding\") * col(\"close_price\")) \\\n",
    "                          .withColumn(\"P/S\", col(\"close_price\") / (col(\"revenues_4q_sum\") / col(\"weighted_shares_outstanding\")))\\\n",
    "                          .withColumn(\"P/E\", col(\"close_price\") / col(\"EPS\")) \\\n",
    "                          .withColumn(\"P/B\", col(\"close_price\") / col(\"BVPS\")) \\\n",
    "                          .withColumn(\"EV\", col(\"market_cap\") + col(\"long_term_debt\")) \\\n",
    "                          .withColumn(\"EV/EBITDA\", col(\"EV\") / col(\"EBIDTA\")) \\\n",
    "                          .withColumn(\"debt_to_market_cap\", col(\"long_term_debt\") / col(\"market_cap\")) \\\n",
    "                          .select(\"ticker\", \"name\", \"type\", \"primary_exchange\", \"market_cap\", \"close_price\", \n",
    "                                  \"EPS\", \"BVPS\",  \"P/S\", \"P/E\", \"P/B\", \"D/E\", \"ROE\", \"ROA\", \"debt_rate\", \"debt_EBIDTA\",\n",
    "                                  \"debt_to_market_cap\", \"current_liquidity_rate\", \"EV\", \"EBIDTA\", \"EV/EBITDA\", \"net_profit_margin\", \n",
    "                                  \"continuous_dividend_years\", \"net_income_3_year_cagr_pct\", \"date\", \"fiscal_period\", \"fiscal_year\", \"start_date\", \"end_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c29095-fbc3-4d24-a49f-4833692260e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "display(final_df.filter(final_df.date == '2025-03-17'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d890cd-4662-4c5a-bd5b-a5d4363ab3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeCondition = \"\"\"target.ticker = source.ticker AND \n",
    "                    target.date = source.date\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04bfaea0-5b98-43ec-b6ea-687a6205b3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incrementalLoadDelta(input_df=final_df, databaseName=\"engineering_presentation\", tableName=\"tickers_indicators\", \n",
    "                     folderPath=presentation_folder_path, partitionField=\"ticker\",mergeCondition=mergeCondition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdca0143-29dc-404c-9f9d-ad0e756e4eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM engineering_presentation.tickers_indicators;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4721398614228095,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "count_tickers_indicators",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
